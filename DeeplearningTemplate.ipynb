{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path, PosixPath\n",
    "import pandas.plotting\n",
    "import sys\n",
    "import cv2 as cv\n",
    "import tensorflow as tf \n",
    "#import git # pip install GitPython\n",
    "import os\n",
    "import matplotlib.pyplot as plt # pip install matplotlib\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====>GPU Available:  True\n",
      "v1.14.0-rc1-22-gaf24dc9 1.14.0\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "print(\"=====>GPU Available: \", tf.test.is_gpu_available())\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "tf.config.experimental.list_physical_devices()\n",
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Log Data\n",
    "print('Modification:')\n",
    "modification = input() # Was wurde verÃ¤ndert\n",
    "print('User:')\n",
    "user = input()#git.util.get_user_id() # Bearbeiter, Sollte automatisch gelesen werden \n",
    "try:\n",
    "    repo = git.Repo()\n",
    "    branch = repo.active_branch # Projekt, Sollte automatisch gelesen werden\n",
    "    lastCommit = repo.head.commit.hexsha #\"ca324dadsa\" # Stand, Sollte automatisch gelesen werden\n",
    "except:\n",
    "    print('Branch:')\n",
    "    \n",
    "    branch = input()\n",
    "    lastCommit = ''\n",
    "env = \"Hal9k\" # Test umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Config\n",
    "samplesize = 1000 # Datensatz\n",
    "split = 0.8  # Testdatensatz in % => 0.8 = 80% testdata\n",
    "BATCH_SIZE = 80\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "## Load Images\n",
    "replaced with the code form Helena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImg(paths, dimensions:int, output_size:int, sync:bool, aug:list=['all']): \n",
    "    images = {}\n",
    "    for p in paths:\n",
    "        img = cv.imread(str(p))\n",
    "        img2= cv.resize(img,dsize=(IMG_HEIGHT,IMG_WIDTH), interpolation = cv.INTER_CUBIC)\n",
    "        #Numpy array\n",
    "        images[p.stem] = np.asarray(img2)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showBatch(image_batch, label_batch, prediction=None):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    for n in range(20):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        ax.imshow(image_batch[n])\n",
    "        if prediction is not None:\n",
    "            plt.title(f'{CLASS_NAMES[int(label_batch[n])]} | {CLASS_NAMES[int(np.argmax(prediction[n]))]}')\n",
    "        else:\n",
    "            plt.title(f'{CLASS_NAMES[int(label_batch[n])]}')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Labels / Classes /  Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unicycle', 'giant', 'trek', 'cube', 'canyon', 'cannondale']\n"
     ]
    }
   ],
   "source": [
    "## Get Classes form Folder\n",
    "data_dir = Path(os.path.join(Path.home(), 'Pictures/Pictures_Bicycles'))\n",
    "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
    "CLASS_NAMES = []\n",
    "for f in np.array([item.name for item in data_dir.glob('Training/*')]):\n",
    "    name = f.split('_')[0]\n",
    "    if name not in CLASS_NAMES :\n",
    "        CLASS_NAMES.append(name)\n",
    "print (CLASS_NAMES)\n",
    "\n",
    "\n",
    "\n",
    "def createBatches(l, n):\n",
    "    # looping till length l \n",
    "    nl = []\n",
    "    for i in range(0, len(l), n):  \n",
    "        nl.append(l[i:i + n]) \n",
    "    return nl\n",
    "\n",
    "def train_val_split(keys: list): ## Uses split form model config\n",
    "    train = int(len(keys)*split)\n",
    "    print (f'Trainsize: {train}')\n",
    "    return keys[:train], keys[train:]\n",
    "\n",
    "def getLabels(keys):\n",
    "    labels = []\n",
    "    for path in keys:\n",
    "        if str(path).startswith('uni'):\n",
    "            folder = str(path).split('_')[0]\n",
    "        else:\n",
    "            folder = str(path).split('_')[0] # 0 = manifacturer, 1 = type\n",
    "        labels.append(float(CLASS_NAMES.index(folder)))\n",
    "    return labels\n",
    "\n",
    "def getImages(img, keys):\n",
    "    images = []\n",
    "    for k in keys:\n",
    "        images.append(img[k])\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataset(train_path, test_path):\n",
    "    # Prepare list with Image Paths\n",
    "    train_paths = list(train_path.glob('*.jpg'))\n",
    "    test_paths = list(test_path.glob('*.jpg'))\n",
    "    \n",
    "    ## Load Images\n",
    "    raw_train_images = loadImg(train_paths, IMG_HEIGHT, samplesize, True) \n",
    "    raw_test_images = loadImg(test_paths, IMG_HEIGHT, 0, True) # Load images with just resize\n",
    "    \n",
    "    # Gen Train and Test Dataset\n",
    "    train_keys = list(raw_train_images.keys()) \n",
    "    rnd.shuffle(train_keys) ## Shuffle traings dataset\n",
    "    train, val = train_val_split(train_keys) # Spilt dataset for train and validation\n",
    "    # Change dict to list \n",
    "    train_labels = getLabels(train)\n",
    "    train_images = getImages(raw_train_images, train)\n",
    "    val_labels = getLabels(val)\n",
    "    val_images = getImages(raw_train_images, val)\n",
    "    \n",
    "    \n",
    "    #Gen Test Dataset\n",
    "    test = list(raw_test_images.keys())\n",
    "    test_labels = getLabels(test)\n",
    "    test_images = getImages(raw_test_images, test)\n",
    "    \n",
    "        \n",
    "    print(f'Train {len(train_labels)}, {len(train_images)};  Val {len(val_labels)} {len(val_images)};  Test {len(test_labels)}, {len(test_images)}')\n",
    "    \n",
    "    return (createBatches(train_images, BATCH_SIZE), createBatches(train_labels,BATCH_SIZE)), (createBatches(val_images, int(BATCH_SIZE*(1.05-split))), createBatches(val_labels,int(BATCH_SIZE*(1.05-split)))),(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainsize: 340\n",
      "Train 340, 340;  Val 85 85;  Test 85, 85\n"
     ]
    }
   ],
   "source": [
    "train, val, test = prepareDataset(Path(os.path.join(data_dir, 'Training')), Path(os.path.join(data_dir, 'Test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 5, 2 5, 2 85\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(train)} {len(train[0])}, {len(val)} {len(val[0])}, {len(test)} {len(test[0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Code\n",
    "## Build and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               25690240  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 25,691,462\n",
      "Trainable params: 25,691,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Batch: 0\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 1682.7897 - acc: 0.8184\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 4s 52ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 4s 52ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 0.0000e+00 - acc: 1.0000\n",
      "5/5 - 0s - loss: 39.4269 - acc: 0.8500\n",
      "Batch: 1\n",
      "Epoch 1/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 24.7481 - acc: 0.9658\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 4s 51ms/step - loss: 2.4550e-08 - acc: 1.0000\n",
      "Epoch 3/10\n",
      " 4/80 [>.............................] - ETA: 4s - loss: 2.2352e-08 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ef092e20d7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mtlbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m           \u001b[0;31m# `ins` can be callable in tf.distribute.Strategy + eager case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m           \u001b[0mactual_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.direnv/python-3.7.3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Your Code\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
    "    ## Network Layers\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(CLASS_NAMES), activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "## Start Timer \n",
    "start = time.time()\n",
    "for n in range(len(train[0])):\n",
    "    print(f'Batch: {n}')\n",
    "    # Train\n",
    "    tibatch = [train[0][n]]\n",
    "    tlbatch = [train[1][n]]\n",
    "    model.fit(tibatch, tlbatch, epochs=10, shuffle=True, steps_per_epoch=BATCH_SIZE)\n",
    "    \n",
    "    #val\n",
    "    vibatch = [val[0][n]]\n",
    "    vlbatch = [val[1][n]]\n",
    "    test_loss, test_acc = model.evaluate(vibatch, vlbatch, verbose=2, steps=5)\n",
    "    \n",
    "## End Timer\n",
    "end = time.time()\n",
    "runtime = end-start\n",
    "maxAcc = test_acc\n",
    "maxLoss = test_loss\n",
    "\n",
    "print('\\nTest accuracy: {}, \\nRuntime: {}'.format(test_acc, runtime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test Images and Print results\n",
    "real value | prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick 20 random images form trainset\n",
    "test_images, test_labels = [], []\n",
    "ids = rnd.sample(range(0, len(test[0])), 20)\n",
    "for i in range(20):\n",
    "    test_images.append(test[0][ids[i]])\n",
    "    test_labels.append(test[1][ids[i]])\n",
    "    \n",
    "ibatch = [test_images]\n",
    "predictions = model.predict(ibatch, steps=2)\n",
    "showBatch(test_images, test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validate Accuracy based on Testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for pred in predictions:\n",
    "preds = list(np.argmax(pred) for pred in predictions)\n",
    "print(preds)\n",
    "validatedTestAcc = 0\n",
    "for i in range(len(test_labels)):\n",
    "    print(f'Real: {test_labels[i]} => Est: {preds[i]}, Value: {max(predictions[i])}')\n",
    "    if int(test_labels[i]) == preds[i]:\n",
    "        validatedTestAcc += 1\n",
    "validatedTestAcc = validatedTestAcc / len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use creds to create a client to interact with the Google Drive API\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(os.path.join(Path.home(),'client-secret.json'), scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# File and WorkSheet \n",
    "sheet = client.open(\"Model Evaluation\").sheet1\n",
    "\n",
    "# Read row form sheet\n",
    "# readval = sheet.row_values(1)\n",
    "\n",
    "sheet.append_row([modification, branch, user, lastCommit, str(samplesize), str(split), env, str(runtime), str(maxAcc), str(maxLoss), str(validatedTestAcc), str(BATCH_SIZE), str(IMG_HEIGHT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
